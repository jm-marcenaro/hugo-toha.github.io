[{"categories":null,"contents":"","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://hugo-toha.github.io/notes/go/basic/_index.bn/","summary":"","tags":null,"title":"Go বেসিক"},{"categories":null,"contents":" Hello World A sample go program is show here.\npackage main import \u0026#34;fmt\u0026#34; func main() { message := greetMe(\u0026#34;world\u0026#34;) fmt.Println(message) } func greetMe(name string) string { return \u0026#34;Hello, \u0026#34; + name + \u0026#34;!\u0026#34; } Run the program as below:\n$ go run hello.go Variables Normal Declaration:\nvar msg string msg = \u0026#34;Hello\u0026#34; Shortcut:\nmsg := \u0026#34;Hello\u0026#34; Constants const Phi = 1.618 ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://hugo-toha.github.io/notes/go/basic/introduction/","summary":" Hello World A sample go program is show here.\npackage main import \u0026#34;fmt\u0026#34; func main() { message := greetMe(\u0026#34;world\u0026#34;) fmt.Println(message) } func greetMe(name string) string { return \u0026#34;Hello, \u0026#34; + name + \u0026#34;!\u0026#34; } Run the program as below:\n$ go run hello.go Variables Normal Declaration:\nvar msg string msg = \u0026#34;Hello\u0026#34; Shortcut:\nmsg := \u0026#34;Hello\u0026#34; Constants const Phi = 1.618 ","tags":null,"title":"Introduction"},{"categories":null,"contents":" Strings str := \u0026#34;Hello\u0026#34; Multiline string\nstr := `Multiline string` Numbers Typical types\nnum := 3 // int num := 3. // float64 num := 3 + 4i // complex128 num := byte(\u0026#39;a\u0026#39;) // byte (alias for uint8) Other Types\nvar u uint = 7 // uint (unsigned) var p float32 = 22.7 // 32-bit float Arrays // var numbers [5]int numbers := [...]int{0, 0, 0, 0, 0} Pointers func main () { b := *getPointer() fmt.Println(\u0026#34;Value is\u0026#34;, b) func getPointer () (myPointer *int) { a := 234 return \u0026amp;a a := new(int) *a = 234 Pointers point to a memory location of a variable. Go is fully garbage-collected.\nType Conversion i := 2 f := float64(i) u := uint(i) Slice slice := []int{2, 3, 4} slice := []byte(\u0026#34;Hello\u0026#34;) ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://hugo-toha.github.io/notes/go/basic/types/","summary":"Strings str := \u0026#34;Hello\u0026#34; Multiline string\nstr := `Multiline string` Numbers Typical types\nnum := 3 // int num := 3. // float64 num := 3 + 4i // complex128 num := byte(\u0026#39;a\u0026#39;) // byte (alias for uint8) Other Types\nvar u uint = 7 // uint (unsigned) var p float32 = 22.7 // 32-bit float Arrays // var numbers [5]int numbers := [...]int{0, 0, 0, 0, 0} Pointers func main () { b := *getPointer() fmt.","tags":null,"title":"Basic Types"},{"categories":null,"contents":"","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://hugo-toha.github.io/notes/go/advanced/_index.bn/","summary":"","tags":null,"title":"অ্যাডভান্সড"},{"categories":null,"contents":" Condition if day == \u0026#34;sunday\u0026#34; || day == \u0026#34;saturday\u0026#34; { rest() } else if day == \u0026#34;monday\u0026#34; \u0026amp;\u0026amp; isTired() { groan() } else { work() } if _, err := doThing(); err != nil { fmt.Println(\u0026#34;Uh oh\u0026#34;) Switch switch day { case \u0026#34;sunday\u0026#34;: // cases don\u0026#39;t \u0026#34;fall through\u0026#34; by default! fallthrough case \u0026#34;saturday\u0026#34;: rest() default: work() } Loop for count := 0; count \u0026lt;= 10; count++ { fmt.Println(\u0026#34;My counter is at\u0026#34;, count) } entry := []string{\u0026#34;Jack\u0026#34;,\u0026#34;John\u0026#34;,\u0026#34;Jones\u0026#34;} for i, val := range entry { fmt.Printf(\u0026#34;At position %d, the character %s is present\\n\u0026#34;, i, val) n := 0 x := 42 for n != x { n := guess() } ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://hugo-toha.github.io/notes/go/basic/flow-control/","summary":"Condition if day == \u0026#34;sunday\u0026#34; || day == \u0026#34;saturday\u0026#34; { rest() } else if day == \u0026#34;monday\u0026#34; \u0026amp;\u0026amp; isTired() { groan() } else { work() } if _, err := doThing(); err != nil { fmt.Println(\u0026#34;Uh oh\u0026#34;) Switch switch day { case \u0026#34;sunday\u0026#34;: // cases don\u0026#39;t \u0026#34;fall through\u0026#34; by default! fallthrough case \u0026#34;saturday\u0026#34;: rest() default: work() } Loop for count := 0; count \u0026lt;= 10; count++ { fmt.Println(\u0026#34;My counter is at\u0026#34;, count) } entry := []string{\u0026#34;Jack\u0026#34;,\u0026#34;John\u0026#34;,\u0026#34;Jones\u0026#34;} for i, val := range entry { fmt.","tags":null,"title":"Flow Control"},{"categories":null,"contents":" Condition if day == \u0026#34;sunday\u0026#34; || day == \u0026#34;saturday\u0026#34; { rest() } else if day == \u0026#34;monday\u0026#34; \u0026amp;\u0026amp; isTired() { groan() } else { work() } if _, err := doThing(); err != nil { fmt.Println(\u0026#34;Uh oh\u0026#34;) ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://hugo-toha.github.io/notes/go/advanced/files/","summary":" Condition if day == \u0026#34;sunday\u0026#34; || day == \u0026#34;saturday\u0026#34; { rest() } else if day == \u0026#34;monday\u0026#34; \u0026amp;\u0026amp; isTired() { groan() } else { work() } if _, err := doThing(); err != nil { fmt.Println(\u0026#34;Uh oh\u0026#34;) ","tags":null,"title":"File Manipulation"},{"categories":null,"contents":" Variable NAME=\u0026#34;John\u0026#34; echo $NAME echo \u0026#34;$NAME\u0026#34; echo \u0026#34;${NAME} Condition if [[ -z \u0026#34;$string\u0026#34; ]]; then echo \u0026#34;String is empty\u0026#34; elif [[ -n \u0026#34;$string\u0026#34; ]]; then echo \u0026#34;String is not empty\u0026#34; fi ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://hugo-toha.github.io/notes/bash/basic/","summary":" Variable NAME=\u0026#34;John\u0026#34; echo $NAME echo \u0026#34;$NAME\u0026#34; echo \u0026#34;${NAME} Condition if [[ -z \u0026#34;$string\u0026#34; ]]; then echo \u0026#34;String is empty\u0026#34; elif [[ -n \u0026#34;$string\u0026#34; ]]; then echo \u0026#34;String is not empty\u0026#34; fi ","tags":null,"title":"Bash Variables"},{"categories":null,"contents":"Project Overview Welcome back! In this post, we\u0026rsquo;ll delve into the severe drought that affected Buenos Aires Province in Argentina, in 2023, using the CHIRPS dataset and the Google Earth Engine (GEE) Python API.\nAs detailed in the GEE catalog, CHIRPS—short for Climate Hazards Group InfraRed Precipitation with Station data—is a 30+ year quasi-global rainfall dataset. This dataset integrates satellite imagery with in-situ station data at a 0.05° resolution to generate gridded rainfall at daily temporal resolution. This dataset is invaluable for trend analysis and seasonal drought monitoring.\nBy the end of this article, we’ll produce a series of maps showcasing:\nThe annual cumulative precipitation for the year of interest.\nThe mean annual cumulative precipitation for the entire CHIRPS record.\nAn anomaly map highlighting the difference between the first two maps.\nTo ensure this project is reproducible, all the code is available in my GitHub repository. Feel free to explore, run the code, and share your thoughts or suggestions!\nAnalysis We\u0026rsquo;ll begin by importing the necessary libraries, initializing Earth Engine, defining the region of interest (ROI), and creating a dictionary to store the image collections for our analysis. This dictionary will contain an image collection for each year. Keep in mind that the CHIRPS dataset provides daily precipitation values with a spatial resolution of approximately 5000 meters.\n# Libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap import matplotlib.colors as colors import geopandas as gpd from datetime import datetime, timedelta import ee import urllib.request import rasterio from rasterio.mask import mask import xarray as xr # Initialize earth engine ee.Initialize() # Read Pvcia. Buenos Aires\u0026#39; shapefile GDF_01 = gpd.read_file(r\u0026#34;GIS/pvcia_bs_as_4326.shp\u0026#34;) # For now, we\u0026#39;ll work with Pvcia. Bs. As\u0026#39; bounding box BB_01 = GDF_01.total_bounds # Define coordinates x_min, y_min, x_max, y_max = BB_01 # Create ee.Geometry ROI = ee.Geometry.Rectangle([x_min, y_min, x_max, y_max]) # Create a dictionary of image collections (one for every year) DICT_CHIRPS_01 = {} for YEAR in range (1981, 2024): DATE_START = f\u0026#34;{YEAR}-01-01\u0026#34; DATE_END = f\u0026#34;{YEAR+1}-01-01\u0026#34; DICT_CHIRPS_01[f\u0026#34;{YEAR}\u0026#34;] = ee.ImageCollection(\u0026#34;UCSB-CHG/CHIRPS/DAILY\u0026#34;) \\ .filterBounds(ROI) \\ .filterDate(DATE_START, DATE_END) Next, we\u0026rsquo;ll create a new dictionary that stores the annual cumulative precipitation for each year. This will involve summing up the daily precipitation values within each year\u0026rsquo;s image collection.\n# Define new dictionary of images with the cumulative sum of each year DICT_CHIRPS_02 = {} for k, C in DICT_CHIRPS_01.items(): DICT_CHIRPS_02[k] = C.sum() To calculate the mean annual precipitation, we first need to create a new image collection that combines all the annual cumulative precipitation images we\u0026rsquo;ve defined previously. Once this collection is assembled, we can compute the mean annual precipitation by taking the average of all the images in the collection. Here\u0026rsquo;s how you can do it:\n# Create new image collection containin the images we\u0026#39;ve defined before (each pixel we\u0026#39;ll have 43 values, 1 for every year of the analysis) # Create empty image collection CHIRPS_IC_01 = ee.ImageCollection([]) for k, I in DICT_CHIRPS_02.items(): # Append image to image collection CHIRPS_IC_01 = IC_01.merge(ee.ImageCollection([I])) # Create new image with the mean of the image collection we\u0026#39; ve defined previously CHIRPS_I_01 = CHIRPS_IC_01.mean() Anomaly is then calculated as the difference between the cumulated precipitation of 2023 and the image we\u0026rsquo;ve just created.\n# Calculate the anomaly as the difference between 2023 cumulative precipitation and the mean annual precipitation CHIRPS_AN_2023 = DICT_CHIRPS_02[\u0026#34;2023\u0026#34;].subtract(CHIRPS_I_01) Now that we\u0026rsquo;ve defined the three images we wanted, we can proceed to create our maps. We\u0026rsquo;ll process the images using Rasterio, but first, we need to download the images locally.\n# Get spatial resolution OS = DICT_CHIRPS_01[\u0026#34;2023\u0026#34;].first().projection().nominalScale().getInfo() # Download images locally for I, DESC in zip([CHIRPS_I_01, CHIRPS_AN_2023, DICT_CHIRPS_02[\u0026#34;2023\u0026#34;]], [\u0026#34;HIST\u0026#34;, \u0026#34;ANOM\u0026#34;, \u0026#34;2023\u0026#34;]): FN = f\u0026#34;PPTAC-CHIRPS-BSAS-{DESC}.tif\u0026#34; # Define the export parameters. url = I.getDownloadURL({ \u0026#34;bands\u0026#34; : [\u0026#34;precipitation\u0026#34;], \u0026#34;scale\u0026#34; : OS, \u0026#34;crs\u0026#34; : \u0026#34;EPSG:4326\u0026#34;, \u0026#34;region\u0026#34; : ROI, \u0026#34;filePerBand\u0026#34;: False, \u0026#34;format\u0026#34; : \u0026#34;GEO_TIFF\u0026#34; }) # Start downloading urllib.request.urlretrieve(url, fr\u0026#34;Output/{FN}\u0026#34;) print(f\u0026#34;{FN} downloaded!\u0026#34;) After writing the images to TIFF files, we\u0026rsquo;ll read them and clip them using the Buenos Aires Province shapefile. This will ensure that pixels outside the region of interest are set to NaN. We\u0026rsquo;ll use Rasterio\u0026rsquo;s mask method to accomplish this.\nDICT_Rs_01 = {} for DESC in [\u0026#34;HIST\u0026#34;, \u0026#34;ANOM\u0026#34;, \u0026#34;2023\u0026#34;]: FN = f\u0026#34;PPTAC-CHIRPS-BSAS-{DESC}.tif\u0026#34; DICT_Rs_01[f\u0026#34;{DESC}\u0026#34;] = rasterio.open(fr\u0026#34;Output/{FN}\u0026#34;) # Clip images by Buenos Aires province shapefile DICT_Rs_02 = {} for k, R in DICT_Rs_01.items(): out_image, out_transform = mask(R, GDF_01.geometry, crop=True, nodata=np.nan) # Store the clipped image DICT_Rs_02[f\u0026#34;{k}\u0026#34;] = out_image[0] Next, we\u0026rsquo;ll define custom color palettes for the cumulative precipitation and precipitation anomaly maps. Additionally, we’ll specify parameters for the maps, such as width, height, and bounds. To provide context, we’ll also include a shapefile with Buenos Aires districts.\n# Define palettes # Cumulated PPT palette pp_hex = [\u0026#39;ffffff\u0026#39;, \u0026#39;d9eafd\u0026#39;, \u0026#39;b3d6fb\u0026#39;, \u0026#39;8ec2f9\u0026#39;, \u0026#39;68aef7\u0026#39;, \u0026#39;439af5\u0026#39;, \u0026#39;2186f3\u0026#39;, \u0026#39;0062f1\u0026#39;, \u0026#39;0051d8\u0026#39;, \u0026#39;0040bf\u0026#39;, \u0026#39;0030a6\u0026#39;, \u0026#39;00208d\u0026#39;, \u0026#39;001174\u0026#39;, \u0026#39;00005b\u0026#39;, \u0026#39;000042\u0026#39;] pp_rgba = [colors.hex2color(\u0026#39;#\u0026#39; + hex_color + \u0026#39;FF\u0026#39;) for hex_color in pp_hex] pp_palette = ListedColormap(pp_rgba) # Anomaly PPT palette an_hex = [\u0026#39;a50026\u0026#39;, \u0026#39;d73027\u0026#39;, \u0026#39;f46d43\u0026#39;, \u0026#39;fdae61\u0026#39;, \u0026#39;fee08b\u0026#39;, \u0026#39;ffffff\u0026#39;, \u0026#39;d9ef8b\u0026#39;, \u0026#39;a6d96a\u0026#39;, \u0026#39;66bd63\u0026#39;, \u0026#39;1a9850\u0026#39;, \u0026#39;006837\u0026#39;] an_rgba = [colors.hex2color(\u0026#39;#\u0026#39; + hex_color + \u0026#39;FF\u0026#39;) for hex_color in an_hex] an_palette = ListedColormap(an_rgba) # Define figure parameters # Aspect IMG_RATIO = DICT_Rs_02[\u0026#34;2023\u0026#34;].shape[1] / DICT_Rs_02[\u0026#34;2023\u0026#34;].shape[0] # Define width and height W = 7 H = W * IMG_RATIO # Set figure bounds BOUNDS = DICT_Rs_01[\u0026#34;2023\u0026#34;].bounds # Set fontsizes T_FS = 14 AX_FS = 12 # Read complimentary shapefile with Buenos Aires province districts to add context GDF_02 = gpd.read_file(r\u0026#34;GIS/limites_partidos.shp\u0026#34;) Finally we\u0026rsquo;ll use the parameters we\u0026rsquo;ve defined and create our maps by using matplotlibs capabilities as follows:\nfig, ax = plt.subplots(1, 3, figsize=(3*W, H)) # Mean annual rainfall _ = ax[0].imshow(DICT_Rs_02[\u0026#34;HIST\u0026#34;], vmin=100, vmax=1500, cmap=pp_palette, extent=(BOUNDS.left, BOUNDS.right, BOUNDS.bottom, BOUNDS.top), aspect=1/IMG_RATIO) ax[0].set_title(f\u0026#34;Mean Annual Rainfall (1981 to 2023)\u0026#34;, fontweight=\u0026#34;bold\u0026#34;, fontsize=T_FS) CB_0 = plt.colorbar(_, extend=\u0026#34;both\u0026#34;, ticks=np.arange(100, 1600, 100)) # annual rainfall 2023 _ = ax[1].imshow(DICT_Rs_02[\u0026#34;2023\u0026#34;], vmin=100, vmax=1500, cmap=pp_palette, extent=(BOUNDS.left, BOUNDS.right, BOUNDS.bottom, BOUNDS.top), aspect=1/IMG_RATIO) ax[1].set_title(f\u0026#34;Annual Rainfall - 2023\u0026#34;, fontweight=\u0026#34;bold\u0026#34;, fontsize=T_FS) CB_1 = plt.colorbar(_, extend=\u0026#34;both\u0026#34;, ticks=np.arange(100, 1600, 100)) # Rainfall anomaly 2023 _ = ax[2].imshow(DICT_Rs_02[\u0026#34;ANOM\u0026#34;], vmin=-300, vmax=300, cmap=an_palette, extent=(BOUNDS.left, BOUNDS.right, BOUNDS.bottom, BOUNDS.top), aspect=1/IMG_RATIO) ax[2].set_title(f\u0026#34;Rainfall Anomaly - 2023\u0026#34;, fontweight=\u0026#34;bold\u0026#34;, fontsize=T_FS) CB_2 = plt.colorbar(_, extend=\u0026#34;both\u0026#34;, ticks=np.arange(-400, 450, 50)) for ax, CB in zip(ax.ravel(), [CB_0, CB_1, CB_2]): GDF_01.plot(ax=ax, facecolor=\u0026#34;none\u0026#34;, edgecolor=\u0026#34;black\u0026#34;, zorder=2, linewidth=2.5, alpha=.9) GDF_02.plot(ax=ax, facecolor=\u0026#34;none\u0026#34;, edgecolor=\u0026#34;black\u0026#34;, zorder=2, linewidth=0.75, alpha=.35) ax.axis(\u0026#34;off\u0026#34;) CB.ax.tick_params(labelsize=AX_FS) CB.set_label(\u0026#34;[mm]\u0026#34;, fontsize=AX_FS) fig.tight_layout() plt.savefig(r\u0026#34;Output/_01.png\u0026#34;) plt.show(); Conclusion This post has come to an end. Our analysis provides spatial insights into the most severe drought areas in Buenos Aires Province for 2023, with the most affected region located in the northwest part of the province. We also identified areas that were less affected and observed general patterns by examining the historical record.\nWe utilized the GEE Python API, along with libraries like GeoPandas, Rasterio, and Matplotlib, to achieve this.\nIn our next post, we’ll focus on extracting daily precipitation values from specific coordinates of interest. We will then compare the 2023 time series with historical data to assess the severity of the drought in greater detail.\n","date":"July 30, 2024","hero":"/images/default-hero.jpg","permalink":"https://hugo-toha.github.io/posts/gee/chirps/chirps_1/","summary":"Project Overview Welcome back! In this post, we\u0026rsquo;ll delve into the severe drought that affected Buenos Aires Province in Argentina, in 2023, using the CHIRPS dataset and the Google Earth Engine (GEE) Python API.\nAs detailed in the GEE catalog, CHIRPS—short for Climate Hazards Group InfraRed Precipitation with Station data—is a 30+ year quasi-global rainfall dataset. This dataset integrates satellite imagery with in-situ station data at a 0.05° resolution to generate gridded rainfall at daily temporal resolution.","tags":null,"title":"GEE Python API and CHIRPS: Analyzing precipitation in Buenos Aires - Part 1"},{"categories":null,"contents":"Project Overview In this post, we continue our exploration of the 2023 severe drought in Buenos Aires province, Argentina. In our previous post, we used the CHIRPS dataset to analyze the extent and impact of the drought. Now, we’ll take our analysis a step further by extracting time series data from specific coordinates within the affected region.\nTo ensure that you can follow along and reproduce the results, all the code used in this analysis is available in my GitHub repository.\nAnalysis We\u0026rsquo;ll start by calling the necessary libraries, initializing GEE, and setting our point of interest—specifically chosen for experiencing the most severe drought conditions during this period.\n# Libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt import ee from tqdm import tqdm # Initialize earth engine ee.Initialize() # Define POI (coordinates of interest) POI = ee.Geometry.Point([-60.92069,-34.57511]) Next up, we\u0026rsquo;ll define a dictionary of image collections, one for each year in the 1981-2023 period. Since we’re working with a point object, the code will return only the values associated with the nearest pixel.\n# Create a dictionary of image collections (one for every year) DICT_CHIRPS_01 = {} for YEAR in range (1981, 2024): DATE_START = f\u0026#34;{YEAR}-01-01\u0026#34; DATE_END = f\u0026#34;{YEAR+1}-01-01\u0026#34; DICT_CHIRPS_01[f\u0026#34;{YEAR}\u0026#34;] = ee.ImageCollection(\u0026#34;UCSB-CHG/CHIRPS/DAILY\u0026#34;) \\ .filterBounds(POI) \\ .filterDate(DATE_START, DATE_END) In the following step, we\u0026rsquo;ll calculate the mean over the POI (even though it’s a single pixel) and append the daily precipitation values to a dictionary of lists, one for each year.\n# Store daily precipitation for each year DICT_CHIRPS_02 = {} # Get spatial resolution OS = DICT_CHIRPS_01[\u0026#34;1981\u0026#34;].first().projection().nominalScale().getInfo() for k, IC in tqdm(DICT_CHIRPS_01.items()): # Compute daily ppt on every image _ = IC.map(lambda image: image.set(\u0026#39;precipitation\u0026#39;,\\ image.reduceRegion(reducer=ee.Reducer.mean(), geometry=POI, scale=OS)\\ .get(\u0026#39;precipitation\u0026#39;) )) # Daily ppt to list P_L = _.aggregate_array(\u0026#39;precipitation\u0026#39;).getInfo() DICT_CHIRPS_02[f\u0026#34;{k}\u0026#34;] = P_L With this dictionary, we’ll then create another dictionary of pandas DataFrames. Each DataFrame will contain daily and cumulative precipitation values for each year.\n# Turn each list into a dataframe DICT_CHIRPS_03 = {} for k, L in DICT_CHIRPS_02.items(): DICT_CHIRPS_03[f\u0026#34;{k}\u0026#34;] = pd.DataFrame(data={\u0026#34;PPT\u0026#34; : L}) # Calculate cumultive PPT DICT_CHIRPS_03[f\u0026#34;{k}\u0026#34;][\u0026#34;CUM\u0026#34;] = DICT_CHIRPS_03[f\u0026#34;{k}\u0026#34;][\u0026#34;PPT\u0026#34;].cumsum() Finally, we can now visualize the results to compare precipitation trends throughout the year and assess the severity of the 2023 drought scenario against historical records.\n# Plot cumulative precipitation of every year fig, ax = plt.subplots(figsize=(15, 5)) for k, DF in DICT_CHIRPS_03.items(): if k != \u0026#34;2023\u0026#34;: ax.plot(DF[\u0026#34;CUM\u0026#34;], c=\u0026#34;gray\u0026#34;, alpha=.4) else: ax.plot(DF[\u0026#34;CUM\u0026#34;], c=\u0026#34;firebrick\u0026#34;, linewidth=2, linestyle=\u0026#34;dashed\u0026#34;, label=\u0026#34;2023\u0026#34;) ax.set_title(\u0026#34;CHIRPS - Cumulative Rainfall at POI (1981-2023)\u0026#34;, fontweight=\u0026#34;bold\u0026#34;, fontsize=14) ax.set_ylabel(\u0026#34;[mm]\u0026#34;, fontweight=\u0026#34;bold\u0026#34;, fontsize=14) ax.set_xlabel(\u0026#34;Day of year\u0026#34;, fontweight=\u0026#34;bold\u0026#34;, fontsize=14) ax.grid(alpha=.5) ax.set_ylim(0, 2000) ax.set_xlim(0, 365) ax.set_xticks(range(0, 360+10, 10)) ax.tick_params(which=\u0026#34;both\u0026#34;, labelsize=12) ax.tick_params(axis=\u0026#39;x\u0026#39;, rotation=-90) _, = plt.plot([], [], label=\u0026#34;Remaining years\u0026#34;, linestyle=\u0026#39;-\u0026#39;, color=\u0026#39;gray\u0026#39;, alpha=.4) plt.legend(handles=[_], loc=\u0026#34;upper left\u0026#34;) ax.legend(loc=\u0026#34;upper left\u0026#34;, fontsize=12) fig.tight_layout() plt.show(); Conclusion As observed, the year started with precipitation values within the expected range. However, around day 80, drought conditions began to develop and persisted throughout the year, culminating in the most severe drought recorded during this period.\nThe mean annual rainfall for the entire period from 1981 to 2023 is 1029.3 mm. In contrast, the total rainfall for 2023 was only 701.1 mm—approximately 300 mm less than the average. This significant deficit in precipitation had severe implications for agriculture, making it crucial to quantify these values to understand and assess the impact on crop yields.\nThis post marks the end of our series on analyzing the 2023 severe drought in Buenos Aires province using the CHIRPS dataset. We specifically focused on a set of coordinates where the drought\u0026rsquo;s impact was most severe. Leveraging the capabilities of the GEE Python API, along with tools like Geopandas, Rasterio, and Matplotlib, we thoroughly examined and visualized the data.\n","date":"July 30, 2024","hero":"/images/default-hero.jpg","permalink":"https://hugo-toha.github.io/posts/gee/chirps/chirps_2/","summary":"Project Overview In this post, we continue our exploration of the 2023 severe drought in Buenos Aires province, Argentina. In our previous post, we used the CHIRPS dataset to analyze the extent and impact of the drought. Now, we’ll take our analysis a step further by extracting time series data from specific coordinates within the affected region.\nTo ensure that you can follow along and reproduce the results, all the code used in this analysis is available in my GitHub repository.","tags":null,"title":"GEE Python API and CHIRPS: Analyzing precipitation in Buenos Aires - Part 2"},{"categories":null,"contents":"Project Overview Greetings! Welcome to the first part of a deep dive into Google Earth Engine (GEE) and its Python API. In this series, we\u0026rsquo;ll explore how to leverage the power of GEE for geospatial analysis, focusing on precipitation forecasting using the Global Forecast System (GFS) dataset.\nGFS is a widely-used weather forecast model developed by NOAA. It provides comprehensive weather data, including temperature, wind, and precipitation forecasts, on a global scale. The model delivers forecasts up to 16 days into the future, making it an invaluable tool for a wide range of applications.\nMoreover, we\u0026rsquo;ll explore the newly released XEE library. XEE combines the well-known xarray library with Google Earth Engine, providing powerful tools for handling and analyzing geospatial data. See more about XEE here.\nTo make this project reproducible, you can access all the code from my GitHub repository. Feel free to check it out, try the code yourself, and leave comments or suggestions.\nBy the end of this tutorial, you\u0026rsquo;ll be able to extract a series of precipitation data at 1-hour intervals and calculate cumulative values for a forecast window of 5 days for your coordinates of interest.\nAnalysis To begin with, we\u0026rsquo;ll import the necessary libraries and set our region of interest (ROI). Since I\u0026rsquo;m from Argentina, I\u0026rsquo;ve chosen region from the city I live in, Buenos Aires, as the focus for this analysis.\n# Libraries import ee import pandas as pd import numpy as np import xarray as xr from datetime import datetime, timedelta from tqdm import tqdm import matplotlib.pyplot as plt import matplotlib.dates as mdates ee.Initialize() #ee.Authenticate() isn\u0026#39;t necessary if you\u0026#39;ve your credentials stored. COORDs = [ [-60.09384640, -33.11803785], [-56.61465669, -33.11803785], [-56.61465669, -35.91630163], [-60.09384640, -35.91630163] ] ROI = ee.Geometry.Polygon(COORDs) Now we\u0026rsquo;ll define our dates of interest. I find it relevant to get the accumulated precipitation from the current date onward. For this, we\u0026rsquo;ll select the simulation that starts at T00, which is the initial run of the day. The GFS model performs four simulations daily at 00:00, 06:00, 12:00, and 18:00 UTC. We\u0026rsquo;ll focus on the T00 run to get the forecast data for our analysis. # Select the simulation launched at T00 to obtain the accumulated precipitation for the current day DATE_START = f\u0026#34;{datetime.strftime(datetime.now(), \u0026#39;%Y-%m-%d\u0026#39;)}T00:00\u0026#34; DATE_END = f\u0026#34;{datetime.strftime(datetime.now(), \u0026#39;%Y-%m-%d\u0026#39;)}T06:00\u0026#34; Now we\u0026rsquo;ll create an image collection using the region and dates of interest that we\u0026rsquo;ve defined previously. For more information on this dataset, you can consult the GEE catalog here. Additionally, we\u0026rsquo;ll select the precipitation band and extract the spatial resolution and projection of the data.\nTo facilitate data management and analysis, we\u0026rsquo;ll convert the image collection to an xarray dataset using the XEE library. This conversion allows us to leverage xarray\u0026rsquo;s powerful capabilities for handling multi-dimensional arrays, making it much easier to manipulate and analyze the dataset. C_01 = ee.ImageCollection(\u0026#34;NOAA/GFS0P25\u0026#34;).map(lambda image: image.clip(ROI))\\ .filterDate(DATE_START, DATE_END)\\ .filterMetadata(\u0026#34;forecast_hours\u0026#34;, \u0026#34;greater_than\u0026#34;, 0) # Select band of interest C_01 = C_01.select([\u0026#34;total_precipitation_surface\u0026#34;]) # Get the spatial resolution OS = C_01.first().projection().nominalScale().getInfo() # print(f\u0026#34;Original scale: {OS:.1f} m\u0026#34;) # Get projection data PROJ = C_01.first().select(0).projection() # Turn the image collection object into a xarray dataset DS_01 = xr.open_dataset(C_01, engine=\u0026#39;ee\u0026#39;, crs=\u0026#34;EPSG:4326\u0026#34;, projection=PROJ, geometry=ROI) Now we\u0026rsquo;ll structure our dataset. First, we\u0026rsquo;ll rename the precipitation band to something more descriptive. Next, we\u0026rsquo;ll slice the dataset to include only the first 120 records. This is because GFS data provides hourly frequency forecasts for the first 5 days. For longer-term forecasts, the data shifts to a 3-hour frequency, which we can exclude since we are\u0026rsquo;nt interested.\nAfter defining the initial parameters, we\u0026rsquo;ll create a pandas date range starting from DATE_START and spanning 120 hours with an hourly frequency.\nNext, we update the xarray dataset by assigning our date range FyH as the new temporal coordinate, replacing the original time dimension. We then drop the old time variable and introduce a new data array, FH (forecast hours), which indexes each forecast hour from 1 to 120. This reorganization makes the dataset more intuitive and easier to work with for further analysis and visualization.\n# Rename band DS_01 = DS_01.rename({\u0026#34;total_precipitation_surface\u0026#34; : \u0026#34;PPT\u0026#34;}) # Filter first 120 registers DS_01 = DS_01.isel(time=slice(0, 120)) # Create a pandas daterange starting from DATE_START and spanning 120 hours FyH = pd.date_range(start=DATE_START, freq=\u0026#34;1H\u0026#34;, periods=120+1)[1:] DS_01 = DS_01.assign_coords(FyH=(\u0026#34;time\u0026#34;, FyH)) DS_01 = DS_01.swap_dims({\u0026#34;time\u0026#34; : \u0026#34;FyH\u0026#34;}) DS_01 = DS_01.drop_vars(\u0026#34;time\u0026#34;) DS_01[\u0026#34;FH\u0026#34;] = xr.DataArray(np.arange(1, 121), dims=\u0026#34;FyH\u0026#34;) Now comes a tricky part. GFS data reports precipitation as cumulative values, resetting every 6 hours. To extract hourly precipitation values, we first create a new data array, H, using the modulo operator to identify these 6-hour periods. We then calculate hourly increments by finding the difference between consecutive precipitation values (PPT_D). To handle the 6-hour reset accurately, we use the where method as follows: PPT_D remains unchanged except where the previous H value was zero, in which case PPT_D is set equal to PPT. This ensures correct hourly precipitation data. Finally we compute the cumulative precipitation for the 5 day period. # Modula operator. Possible values are 0, 1, 2, 3, 4 and 5 DS_01[\u0026#34;H\u0026#34;] = DS_01[\u0026#34;FH\u0026#34;] % 6 # One hour increments DS_01[\u0026#34;PPT_D\u0026#34;] = DS_01[\u0026#34;PPT\u0026#34;].diff(dim=\u0026#34;FyH\u0026#34;) # PPT_D remains the same except where the previous H value was equal to 0 DS_01[\u0026#34;PPT_D\u0026#34;] = DS_01[\u0026#34;PPT_D\u0026#34;].where(DS_01[\u0026#34;H\u0026#34;].shift(FyH=1) != 0, DS_01[\u0026#34;PPT\u0026#34;]) # Calculate cumulative ppt along the FyH dimension DS_01[\u0026#34;CUMSUM\u0026#34;] = DS_01[\u0026#34;PPT_D\u0026#34;].cumsum(dim=\u0026#34;FyH\u0026#34;) Next, and nearing the end, we\u0026rsquo;ll convert the dataset into a Pandas dataframe. Specifically, we\u0026rsquo;ll extract precipitation data for specific coordinates of interest within the region we\u0026rsquo;ve defined previously. This will yield the precipitation data from the nearest pixel in the dataset to the provided coordinates. LON, LAT = -58.46633, -34.59960 DF_01 = DS_01.sel(lon=LON, lat=LAT, method=\u0026#34;nearest\u0026#34;).to_dataframe().drop(columns={\u0026#34;lon\u0026#34;, \u0026#34;lat\u0026#34;}) Visualization As a final step, we\u0026rsquo;ll create a plot to visualize our results using Matplotlib\u0026rsquo;s capabilities. Below is the final piece of code and the corresponding output from our analysis: fig, ax = plt.subplots(2, 1, figsize=(12, 6), gridspec_kw={\u0026#34;height_ratios\u0026#34; : [1, .6]}, sharex=True) ax[0].bar(DF_01.index, DF_01[\u0026#34;PPT_D\u0026#34;], label=\u0026#34;Hourly Ppt.\u0026#34;, color=\u0026#34;black\u0026#34;, zorder=5, width=.025) ax[0].set_title(f\u0026#34;Hourly precipitation forecast - GFS Model (SIM.: {DATE_START})\u0026#34;, fontweight=\u0026#34;bold\u0026#34;) # Set y range and ticks ax[0].set_ylim(0, 15) ax[0].yaxis.set_ticks(np.arange(0, 15+1, 1)) ax[1].plot(DF_01.index, DF_01[\u0026#34;CUMSUM\u0026#34;], label=\u0026#34;Cumulative Ppt.\u0026#34;, color=\u0026#34;firebrick\u0026#34;, zorder=5) ax[1].set_title(f\u0026#34;Cumulative precipitation forecast - GFS Model (SIM.: {DATE_START})\u0026#34;, fontweight=\u0026#34;bold\u0026#34;) # Set y range and ticks ax[1].set_ylim(0, 50) ax[1].yaxis.set_ticks(np.arange(0, 50+10, 10)) # Figure settings along both axis for i in [0, 1]: ax[i].set_ylabel(\u0026#34;[mm]\u0026#34;) ax[i].legend(loc=\u0026#34;upper left\u0026#34;) ax[i].grid(alpha=.5) ax[i].tick_params(labelbottom=True) ax[i].tick_params(axis=\u0026#34;both\u0026#34;, which=\u0026#34;major\u0026#34;) DATE_FMT = mdates.DateFormatter(\u0026#39;%d-%mT%H\u0026#39;) ax[i].xaxis.set_major_formatter(DATE_FMT) ax[i].xaxis.set_major_locator(mdates.HourLocator(byhour=[0, 6, 12, 18])) ax[i].tick_params(axis=\u0026#34;x\u0026#34;, labelrotation=-90) fig.tight_layout() plt.show(); Conclusion Looks like we might be expecting some heavy rain this weekend, so watch out! I hope this tutorial was useful for understanding how to extract and analyze precipitation data using the GFS dataset in Google Earth Engine and its Python API. In our next article, we\u0026rsquo;ll go one step further by creating spatial maps for a broader region, leveraging the full capabilities of the XEE library alongside new tools like Geopandas and Cartopy. Stay tuned!\n","date":"July 30, 2024","hero":"/images/default-hero.jpg","permalink":"https://hugo-toha.github.io/posts/gee/gfs/gfs_1/","summary":"Project Overview Greetings! Welcome to the first part of a deep dive into Google Earth Engine (GEE) and its Python API. In this series, we\u0026rsquo;ll explore how to leverage the power of GEE for geospatial analysis, focusing on precipitation forecasting using the Global Forecast System (GFS) dataset.\nGFS is a widely-used weather forecast model developed by NOAA. It provides comprehensive weather data, including temperature, wind, and precipitation forecasts, on a global scale.","tags":null,"title":"GEE Python API and Precipitation Forecasting - Part 1"},{"categories":null,"contents":"Project Overview Hello again! Welcome to the continuation of our deep dive into precipitation forecasting using the GFS dataset and the GEE Python API. In our previous post, we demonstrated how to use the GEE Python API along with the XEE library (an integration of GEE and xarray) to forecast precipitation for specific coordinates. This time, we’re going to expand our analysis to cover an entire region. Besides, we\u0026rsquo;ll leverage additional libraries such as Geopandas and Cartopy to create comprehensive spatial maps of precipitation forecasts.\nThe code builds upon what we covered earlier. You can find the Jupyter notebook in my GitHub repository here.\nBy the end of this tutorial, we\u0026rsquo;ll have a map of the cumulative precipitation over a region of interest (ROI) for a 5-day period. Additionally, we\u0026rsquo;ll provide the cumulative and discrete hourly precipitation at a specific location (point of interest or POI) within the ROI. This will enable us to not only get values at a specific location but also gain insight into the spatial pattern of the event.\nAnalysis and Visualization In our previous post, we ended up with two objects: DS_01 and DF_01. DS_01 is an xarray dataset containing the discrete and cumulative precipitation for our region of interest (ROI), while DF_01 is a pandas dataframe containing the same variables but specifically for our point of interest (POI).\nAs a result we created a plot of the variables contained within DF_01 that looked like this:\nNow we\u0026rsquo;ll begin by defining a new object named DS_02 that is an xarray object containing the cumulative precipitation for the 5 day period:\n# Create an image that\u0026#39;s the cumulated precipitation for the entire period. DS_02 = DS_01[\u0026#34;CUMSUM\u0026#34;].isel(FyH=-1) - DS_01[\u0026#34;CUMSUM\u0026#34;].isel(FyH=0) Next, we\u0026rsquo;ll define a geodataframe containing the longitude and latitude of our point of interest (POI): # Create a geodataframe with the coordinates of the point of interest # First we define a dataframe and then we turn it into a geodataframe DF_POI = pd.DataFrame({\u0026#39;LON\u0026#39;: [LON], \u0026#39;LAT\u0026#39;: [LAT]}) GDF_POI = gpd.GeoDataFrame(DF_POI, geometry=gpd.points_from_xy(DF_POI[\u0026#34;LON\u0026#34;], DF_POI[\u0026#34;LAT\u0026#34;]), crs=\u0026#34;EPSG:4326\u0026#34;).drop(columns=[\u0026#34;LON\u0026#34;, \u0026#34;LAT\u0026#34;]) Now, we can make our first map with the following code. Note that we are plotting an xarray object by leveraging the integration between xarray and matplotlib. We selected a discrete blues colorbar and set its range and step. Moreover, we took advantage of some of Cartopy\u0026rsquo;s capabilities, such as setting the map projection and adding coastlines for context. Finally, we\u0026rsquo;ve used Geopandas to plot the location of the point of interest (POI) on the map.\nfig, ax = plt.subplots(1, 1, figsize=(5, 5), subplot_kw={\u0026#39;projection\u0026#39;: ccrs.PlateCarree()}, constrained_layout=True) im =DS_02.plot(x=\u0026#34;lon\u0026#34;, y=\u0026#34;lat\u0026#34;, ax=ax, vmin=0, vmax=50, cmap=\u0026#34;Blues\u0026#34;, add_colorbar=False, levels=11) # POI ax.plot(GDF_POI.geometry.x, GDF_POI.geometry.y, \u0026#39;o\u0026#39;, color=\u0026#34;saddlebrown\u0026#34;, markersize=5, markeredgecolor=\u0026#34;black\u0026#34;, label=\u0026#34;POI\u0026#34;) # Add a title to the whole figure ax.set_title(f\u0026#34;Cumulative precipitation forecast\\n{pd.to_datetime(DATE_START):%Y-%m-%d} to {(pd.to_datetime(DATE_START) + timedelta(days=5)):%Y-%m-%d} (UTC-0)\u0026#34;, fontweight=\u0026#39;bold\u0026#39;) # Add land boundaries ax.add_feature(cf.COASTLINE, linewidth=1.5, edgecolor=\u0026#39;black\u0026#39;) # Grid settings GLs = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True, x_inline=False, y_inline=False, linewidth=.5, color=\u0026#39;gray\u0026#39;, alpha=0.2, linestyle=\u0026#39;--\u0026#39;) GLs.top_labels = False GLs.right_labels = False # Set x and y ticks at 1-degree intervals using MultipleLocator GLs.xlocator = MultipleLocator(0.5) GLs.ylocator = MultipleLocator(0.5) # Create colorbar with specified ticks cbar = fig.colorbar(im, ax=ax, shrink=0.75) cbar.set_ticks(range(0, 50+5, 5)) cbar.set_label(\u0026#34;[mm]\u0026#34;, rotation=-90, labelpad=10) # Legend ax.legend(loc=\u0026#34;upper right\u0026#34;) # Add a footnote to the bottom left corner fig.text(0.02, 0.10, f\u0026#34;GFS Model (SIM.: {DATE_START})\u0026#34;, color=\u0026#39;gray\u0026#39;) plt.show(); Finally, we\u0026rsquo;ll bring everything together and visualize the map alongside the cumulative and discrete precipitation at the POI. We\u0026rsquo;ll use Matplotlib\u0026rsquo;s GridSpec method to create a well-organized layout. The code is extensive but achieves the desired result effectively.\nHere’s the complete code: # Create the main plot using gridspec fig = plt.figure(figsize=(12, 4.5), constrained_layout=True) GS = fig.add_gridspec(nrows=2, ncols=2, width_ratios=[.5, .75]) # Add a title to the whole figure fig.suptitle(f\u0026#34;Cumulative and hourly precipitation forecast {pd.to_datetime(DATE_START):%Y-%m-%d} to {(pd.to_datetime(DATE_START) + timedelta(days=5)):%Y-%m-%d} (UTC-0)\u0026#34;, fontweight=\u0026#39;bold\u0026#39;) # MAP ax_0 = fig.add_subplot(GS[:, 0], projection=ccrs.PlateCarree()) # ROI im =DS_02.plot(x=\u0026#34;lon\u0026#34;, y=\u0026#34;lat\u0026#34;, ax=ax_0, vmin=0, vmax=50, cmap=\u0026#34;Blues\u0026#34;, add_colorbar=False, levels=11) # POI ax_0.plot(GDF_POI.geometry.x, GDF_POI.geometry.y, \u0026#39;o\u0026#39;, color=\u0026#34;saddlebrown\u0026#34;, markersize=5, markeredgecolor=\u0026#34;black\u0026#34;, label=\u0026#34;POI\u0026#34;) ax_0.set_title(f\u0026#34;Cumulative precipitation over ROI\u0026#34;, fontweight=\u0026#34;bold\u0026#34;) # Add land boundaries ax_0.add_feature(cf.COASTLINE, linewidth=1.5, edgecolor=\u0026#39;black\u0026#39;) # Grid settings GLs = ax_0.gridlines(crs=ccrs.PlateCarree(), draw_labels=True, x_inline=False, y_inline=False, linewidth=.5, color=\u0026#39;gray\u0026#39;, alpha=0.2, linestyle=\u0026#39;--\u0026#39;) # Visibility GLs.top_labels = False GLs.right_labels = False # Set x and y ticks at 1-degree intervals using MultipleLocator GLs.xlocator = MultipleLocator(0.5) GLs.ylocator = MultipleLocator(0.5) # Create colorbar with specified ticks cbar = fig.colorbar(im, ax=ax_0, shrink=0.75) cbar.set_ticks(range(0, 50+5, 5)) cbar.set_label(\u0026#34;[mm]\u0026#34;, rotation=-90, labelpad=10) # Legend ax_0.legend(loc=\u0026#34;upper right\u0026#34;) # Discrete Ppt ax_1 = fig.add_subplot(GS[0, 1]) ax_1.bar(DF_01.index, DF_01[\u0026#34;PPT_D\u0026#34;], label=\u0026#34;Hourly Ppt.\u0026#34;, color=\u0026#34;black\u0026#34;, zorder=5, width=.025) ax_1.set_title(f\u0026#34;Hourly precipitation over POI\u0026#34;, fontweight=\u0026#34;bold\u0026#34;) # Set y range and ticks ax_1.set_ylim(0, 15) ax_1.yaxis.set_ticks(np.arange(0, 15+5, 5)) # Set x ticks to false ax_1.tick_params(labelbottom=False) # Cumulative Ppt ax_2 = fig.add_subplot(GS[1, 1]) ax_2.plot(DF_01.index, DF_01[\u0026#34;CUMSUM\u0026#34;], label=\u0026#34;Cumulative Ppt.\u0026#34;, color=\u0026#34;firebrick\u0026#34;, zorder=5) ax_2.set_title(f\u0026#34;Cumulative precipitation over POI\u0026#34;, fontweight=\u0026#34;bold\u0026#34;) # Set y range and ticks ax_2.set_ylim(0, 50) ax_2.yaxis.set_ticks(np.arange(0, 50+10, 10)) ax_2.tick_params(labelbottom=True) DATE_FMT = mdates.DateFormatter(\u0026#39;%d-%mT%H\u0026#39;) ax_2.xaxis.set_major_formatter(DATE_FMT) ax_2.xaxis.set_major_locator(mdates.HourLocator(byhour=[0, 6, 12, 18])) ax_2.tick_params(axis=\u0026#34;x\u0026#34;, labelrotation=-90) # Common properties for ax in [ax_1, ax_2]: ax.set_ylabel(\u0026#34;[mm]\u0026#34;) ax.legend(loc=\u0026#34;upper left\u0026#34;) ax.grid(alpha=.5) ax.tick_params(axis=\u0026#34;both\u0026#34;, which=\u0026#34;major\u0026#34;) ax.xaxis.set_major_locator(mdates.HourLocator(byhour=[0, 6, 12, 18])) # Add a footnote to the bottom left corner fig.text(0.02, 0.02, f\u0026#34;GFS Model (SIM.: {DATE_START})\u0026#34;, color=\u0026#39;gray\u0026#39;) plt.show(); Conclusion In this post, we\u0026rsquo;ve expanded our initial analysis from specific coordinates to a broader region, allowing us to visualize both the spatial distribution and temporal evolution of precipitation. By leveraging the capabilities of GEE, XEE, and additional libraries such as Geopandas and Cartopy, we\u0026rsquo;ve created a comprehensive map and time series plots that provide a detailed understanding of precipitation forecasts.\n","date":"July 30, 2024","hero":"/images/default-hero.jpg","permalink":"https://hugo-toha.github.io/posts/gee/gfs/gfs_2/","summary":"Project Overview Hello again! Welcome to the continuation of our deep dive into precipitation forecasting using the GFS dataset and the GEE Python API. In our previous post, we demonstrated how to use the GEE Python API along with the XEE library (an integration of GEE and xarray) to forecast precipitation for specific coordinates. This time, we’re going to expand our analysis to cover an entire region. Besides, we\u0026rsquo;ll leverage additional libraries such as Geopandas and Cartopy to create comprehensive spatial maps of precipitation forecasts.","tags":null,"title":"GEE Python API and Precipitation Forecasting - Part 2"},{"categories":null,"contents":"Project Overview In this tutorial, we\u0026rsquo;ll explore the capabilities of batch processing in QGIS. Batch processing is incredibly helpful for repetitive tasks that can otherwise consume a lot of time and effort. By automating these tasks, you can focus on more important aspects of your project, increasing both efficiency and productivity.\nLet me set up an example where we\u0026rsquo;ll fully leverage the power of batch processing in QGIS.\nCase Example We\u0026rsquo;ve got a set of 12 MODIS land surface temperature (LST) images for an area of interest. These images are in Kelvin degrees, but before we can perform our analysis in Celsius, we need to first scale the temperature values by a factor of 0.02. After scaling, we\u0026rsquo;ll convert the temperatures from Kelvin to Celsius by subtracting 273.\nIf we were to take the regular approach, we would have to open the Raster Calculator in QGIS and perform these operations 12 times, once for each image. However, with QGIS\u0026rsquo;s batch processing tool, we can automate these tasks and handle all 12 images efficiently in one go.\nFirst we\u0026rsquo;ll load the images into QGIS as shown in the picture below:\nTo perform the scaling and conversion, we will use the Raster Calculator. Note that the Raster Calculator in the traditional menu doesn’t support batch processing, so we need to access it through the Processing Toolbox as shown below:\nA window will open up and we will first click on the option \u0026lsquo;Run as Batch Process\u0026rsquo;. Afterwards, we’ll see the following menu:\nOn this menu, we’ll need to fill out a table with the arguments for the process we are applying to each of our layers. We\u0026rsquo;ll describe the necessary arguments for each of these columns:\nReference layer: let’s start by filling up the \u0026lsquo;Reference layer\u0026rsquo; column. If we click on the \u0026lsquo;Select from open layers\u0026rsquo; option, we’ll be able to choose from a list of all the available layers. As a result, the table will be automatically populated with these layers.\nExpression: Since we have numbered the layers, we will define the expression for the first layer, then use the fill down option for the other layers. Afterwards, manually edit each expression to match the corresponding layer name (this is straightforward as we have numbered the layers). The expression I used was the following: (\u0026ldquo;01@1\u0026rdquo;*0.02)-273\nCell size: By consulting layers metadata, we see that the spatial resolution is 0.0083 (expressed in degrees as layers have a geographical CRS), that\u0026rsquo;s aproximately 925 meters at the equator. We’ll use this value and populate each row by using the fill down option again.\nOutput extent: For the first layer we\u0026rsquo;ll click on the three dots and select the \u0026lsquo;Calculate from layer option\u0026rsquo;. Remaining layers we\u0026rsquo;ll be populated with the same value by using the fill down option.\nOutput CRS: we\u0026rsquo;ll choose the same CRS (EPSG: 4326) of the original layers and project and once again fill down to every other row.\nOutput: Finally, it only remains to define the output for each layer. Click on the three dots for the first layer, set up a prefix (such as C_), and then click on the \u0026lsquo;Fill with numbers\u0026rsquo; option.\nAs a result of all the steps we\u0026rsquo;ve described above, the completed table should look like the picture shown below:\nDont forget to check the \u0026lsquo;Load layers on completion\u0026rsquo; option to visualize your result upon completion!\nWe are now set up to execute the batch process. Click on Run, and if everything works correctly, no warnings should appear in the console. As a result, new layers should appear in the Layers panel, containing the converted temperatures from Kelvin to Celsius.\nConclusion Batch processing might seem daunting at first, but once you get the hang of it, it can be incredibly useful. Understanding the logic behind setting it up will make you more effective and help reduce repetitive and exhausting tasks. I hope this tutorial has been helpful. Stay tuned for more tutorials!\n","date":"July 30, 2024","hero":"/images/default-hero.jpg","permalink":"https://hugo-toha.github.io/posts/qgis/qgis_1/","summary":"Project Overview In this tutorial, we\u0026rsquo;ll explore the capabilities of batch processing in QGIS. Batch processing is incredibly helpful for repetitive tasks that can otherwise consume a lot of time and effort. By automating these tasks, you can focus on more important aspects of your project, increasing both efficiency and productivity.\nLet me set up an example where we\u0026rsquo;ll fully leverage the power of batch processing in QGIS.\nCase Example We\u0026rsquo;ve got a set of 12 MODIS land surface temperature (LST) images for an area of interest.","tags":null,"title":"QGIS: Batch Processing"},{"categories":null,"contents":"Go Notes ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://hugo-toha.github.io/notes/go/_index.bn/","summary":"Go Notes ","tags":null,"title":"Go এর নোট সমূহ"},{"categories":null,"contents":"","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://hugo-toha.github.io/notes/_index.bn/","summary":"","tags":null,"title":"নোট সমূহ"},{"categories":null,"contents":"Bash Notes ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://hugo-toha.github.io/notes/bash/_index.bn/","summary":"Bash Notes ","tags":null,"title":"ব্যাশের নোট সমূহ"}]